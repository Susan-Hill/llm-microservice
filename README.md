# FastAPI LLM Microservice

A containerized LLM inference API built with FastAPI and Docker.

## Goals
- Expose an LLM via a REST API
- Run fully in Docker
- Include production signals (rate limiting, logging)

## Tech Stack
- FastAPI
- Docker & Docker Compose
- Local LLM runtime (Ollama)

## Status
Project setup in progress
